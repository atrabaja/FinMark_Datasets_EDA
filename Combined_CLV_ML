# Import Libraries
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import silhouette_score
import streamlit as st

# Set the environment variable for the number of threads before importing sklearn
os.environ["OMP_NUM_THREADS"] = "5"  # Set this to a lower number to see if it helps

# Load datasets
df1 = pd.read_csv("C:\\Users\\Angel Rabaja\\Downloads\\MO-IT125 Homework_ Data Preprocessing of Machine Learning Solution Project Dataset BSIT-H3103 Mondero, GV., Rabaja, A., Vicente, JB. - cleaned_customer_demographics.csv")
df2 = pd.read_csv("C:\\Users\\Angel Rabaja\\Downloads\\MO-IT125 Homework_ Data Preprocessing of Machine Learning Solution Project Dataset BSIT-H3103 Mondero, GV., Rabaja, A., Vicente, JB. - cleaned_customer_transactions.csv")
df3 = pd.read_csv("C:\\Users\\Angel Rabaja\\Downloads\\MO-IT125 Homework_ Data Preprocessing of Machine Learning Solution Project Dataset BSIT-H3103 Mondero, GV., Rabaja, A., Vicente, JB. - cleaned_social_media_interaction.csv")

# Set display options to show the full DataFrame
pd.set_option('display.max_columns', None)  # Show all columns
pd.set_option('display.expand_frame_repr', False)  # Do not wrap rows

# Check the first few rows of each DataFrame
print("Customer Demographics DataFrame:")
print(df1.head())
print("\nCustomer Transactions DataFrame:")
print(df2.head())
print("\nSocial Media Interactions DataFrame:")
print(df3.head())

# Merge Customer Demographics with Customer Transactions
merged_df = pd.merge(df1, df2, on='Customer ID', how='inner')  # Using 'inner' to keep only matching records
print("\nMerged DataFrame after merging Customer Demographics with Customer Transactions:")
print(merged_df.head())

# Merge the resulting DataFrame with Social Media Interactions
merged_df = pd.merge(merged_df, df3, on='Customer ID', how='inner')
print("\nFinal Merged DataFrame after adding Social Media Interactions:")
print(merged_df.head())

# Check for missing values in the final merged DataFrame
print("\nMissing Values in Final Merged DataFrame:")
missing_values_final = merged_df.isnull().sum()
print(missing_values_final[missing_values_final > 0])  # Display only columns with missing values

# Function to parse dates with multiple formats
def parse_date(date_str):
    for fmt in ('%Y-%m-%d', '%d-%m-%Y', '%m-%d-%Y', '%Y/%m/%d', '%d/%m/%Y', '%m/%d/%Y', '%B %d, %Y', '%d %B %Y'):
        try:
            return pd.to_datetime(date_str, format=fmt)
        except (ValueError, TypeError):
            continue
    return pd.NaT

# Convert date columns
for date_col in ['Sign Up Date', 'Transaction Date', 'Interaction Date']:
    if date_col in merged_df.columns:
        merged_df[date_col] = merged_df[date_col].apply(parse_date)

# Ensure 'Transaction Date' is in datetime format
merged_df['Transaction Date'] = pd.to_datetime(merged_df['Transaction Date'])

# Filter out data for RFM analysis
latest_date = merged_df['Transaction Date'].max() + pd.Timedelta(days=1)

# Calculate RFM metrics
if 'Transaction Date' in merged_df.columns and 'Amount' in merged_df.columns:
    # Set a reference date for recency calculation
    reference_date = merged_df['Transaction Date'].max()

    # Calculate Recency, Frequency, and Monetary (RFM)
    rfm_df = merged_df.groupby('Customer ID').agg({
        'Transaction Date': lambda x: (reference_date - x.max()).days,  # Recency
        'Customer ID': 'count',  # Frequency
        'Amount': 'sum'  # Monetary
    }).rename(columns={
        'Transaction Date': 'Recency',
        'Customer ID': 'Frequency',
        'Amount': 'Monetary'
    }).reset_index()

    print("\nRFM Metrics:")
    print(rfm_df.head())

    # Apply log transformation to Recency, Frequency, and Monetary
    if 'Monetary' in rfm_df.columns:
        rfm_df['Recency_log'] = np.log1p(rfm_df['Recency'])
        rfm_df['Frequency_log'] = np.log1p(rfm_df['Frequency'])
        rfm_df['Monetary_log'] = np.log1p(rfm_df['Monetary'])

        # Print transformed columns to verify
        print("RFM DataFrame with log transformations:")
        print(rfm_df[['Recency_log', 'Frequency_log', 'Monetary_log']].head())
    else:
        print("Error: 'Monetary' column not found in rfm_df.")

    # Split data into training and testing sets
    X = rfm_df[['Recency', 'Frequency', 'Monetary']]
    y = rfm_df['Monetary']  # Assuming we want to predict the Monetary value

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Create a Random Forest Regressor pipeline
    model = Pipeline(steps=[
        ('scaler', StandardScaler()), 
        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))
    ])

    # Train the model
    model.fit(X_train, y_train)

    # Predict and evaluate
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f'Mean Absolute Error: {mae}')
    print(f'R^2 Score: {r2}')

else:
    print("Required columns for RFM calculation are missing.")

# Check if the 'Age' column exists
if 'Age' in df1.columns:
    # We can use Age directly as a feature
    print("Using Age column for feature engineering.")

    # Re-merge the datasets to include the Age feature
    merged_df = pd.merge(merged_df, df1[['Customer ID', 'Age']], on='Customer ID', how='inner')
else:
    print("Error: 'Age' column not found in Customer Demographics data.")

# Calculate Frequency of Transactions
transaction_counts = df2.groupby('Customer ID')['Transaction Date'].count().reset_index(name='Transaction Frequency')

# Calculate Average Transaction Value
average_transaction_value = df2.groupby('Customer ID')['Amount'].mean().reset_index(name='Average Transaction Value')

# Get the Last Purchase Date
last_purchase_date = df2.groupby('Customer ID')['Transaction Date'].max().reset_index(name='Last Purchase Date')

# Merge these new features into the merged DataFrame
merged_df = merged_df.merge(transaction_counts, on='Customer ID', how='left')
merged_df = merged_df.merge(average_transaction_value, on='Customer ID', how='left')
merged_df = merged_df.merge(last_purchase_date, on='Customer ID', how='left')

# Calculate Time since Last Purchase
if 'Last Purchase Date' in merged_df.columns:
    merged_df['Last Purchase Date'] = pd.to_datetime(merged_df['Last Purchase Date'])
    merged_df['Time Since Last Purchase'] = (pd.to_datetime('today') - merged_df['Last Purchase Date']).dt.days

# Display the updated DataFrame
print("\nMerged DataFrame with additional features:")
print(merged_df.head())

# Select RFM features for clustering
rfm_clustering_data = rfm_df[['Recency', 'Frequency', 'Monetary']]

# Determine the optimal number of clusters using the Elbow method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(rfm_clustering_data)
    wcss.append(kmeans.inertia_)

# Plot the Elbow curve to determine the optimal number of clusters
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), wcss, marker='o', color='b', linestyle='--')
plt.title('Elbow Method for Optimal Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS (Within-cluster sum of squares)')
plt.grid(True)
plt.show()

# Based on the elbow plot, determine the optimal number of clusters
optimal_clusters = 4

# Perform KMeans clustering with the chosen number of clusters
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
rfm_df['Cluster'] = kmeans.fit_predict(rfm_clustering_data)

# Visualize the clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Recency', y='Monetary', hue='Cluster', data=rfm_df, palette='Set1', s=100, alpha=0.6)
plt.title(f'KMeans Clustering of Customers (n={optimal_clusters})')
plt.xlabel('Recency')
plt.ylabel('Monetary')
plt.legend(title='Cluster', loc='upper right')
plt.show()

# Visualizing RFM Distribution
plt.figure(figsize=(12, 8))

# Recency Distribution
plt.subplot(2, 2, 1)
sns.histplot(rfm_df['Recency'], kde=True, color='blue', bins=30)
plt.title('Distribution of Recency')

# Frequency Distribution
plt.subplot(2, 2, 2)
sns.histplot(rfm_df['Frequency'], kde=True, color='green', bins=30)
plt.title('Distribution of Frequency')

# Monetary Distribution
plt.subplot(2, 2, 3)
sns.histplot(rfm_df['Monetary'], kde=True, color='red', bins=30)
plt.title('Distribution of Monetary')

# Log-transformed Recency Distribution
plt.subplot(2, 2, 4)
sns.histplot(rfm_df['Recency_log'], kde=True, color='purple', bins=30)
plt.title('Log-transformed Recency Distribution')

plt.tight_layout()
plt.show()

# Silhouette Score to evaluate clustering quality
sil_score = silhouette_score(rfm_clustering_data, rfm_df['Cluster'])
print(f"Silhouette Score for the clustering: {sil_score:.3f}")

# 1. Hypothesis Testing for Customer Segments (T-test/ANOVA)
# We will check if high and low-frequency customers differ in their CLV (Monetary)

# Create two customer segments (e.g., high frequency vs low frequency)
high_freq = rfm_df[rfm_df['Frequency'] > rfm_df['Frequency'].median()]
low_freq = rfm_df[rfm_df['Frequency'] <= rfm_df['Frequency'].median()]

# Hypothesis Testing: Test the correlation between 'Recency' and 'Monetary'
# Null Hypothesis: There is no significant correlation between Recency and Monetary

# Perform Pearson correlation test
correlation, p_value = stats.pearsonr(rfm_df['Recency'], rfm_df['Monetary'])

print(f"Correlation between Recency and Monetary: {correlation}")
print(f"P-value for the correlation test: {p_value}")

# Decision Rule: If p-value < 0.05, we reject the null hypothesis (there is a significant relationship)

if p_value < 0.05:
    print("Reject the null hypothesis: There is a significant relationship between Recency and Monetary.")
else:
    print("Fail to reject the null hypothesis: There is no significant relationship between Recency and Monetary.")

# Visualization: Scatter plot with regression line for Recency vs Monetary
plt.figure(figsize=(10, 6))
sns.regplot(x='Recency', y='Monetary', data=rfm_df, scatter_kws={'color': 'blue'}, line_kws={'color': 'red'})
plt.title('Relationship between Recency and Monetary', fontsize=16)
plt.xlabel('Recency (Days)', fontsize=14)
plt.ylabel('Monetary (Total Spend)', fontsize=14)
plt.grid(True)
plt.show()
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS (Within-Cluster Sum of Squares)')
plt.show()

# Based on the elbow plot, let's assume the optimal number of clusters is 3 (or another value from your analysis)
optimal_clusters = 3
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)

# Fit KMeans clustering model to RFM data
rfm_df['Cluster'] = kmeans.fit_predict(rfm_clustering_data)

# Display the clustering result
print("\nRFM Clusters assigned to each customer:")
print(rfm_df[['Customer ID', 'Cluster']].head())

# Evaluate clustering quality using Silhouette Score
sil_score = silhouette_score(rfm_clustering_data, rfm_df['Cluster'])
print(f'Silhouette Score for the clustering: {sil_score}')

# Visualize the clustering result
plt.figure(figsize=(10, 6))
sns.scatterplot(data=rfm_df, x='Recency', y='Frequency', hue='Cluster', palette='Set1', s=100, marker='o')
plt.title('RFM Clustering Results')
plt.xlabel('Recency')
plt.ylabel('Frequency')
plt.legend(title='Cluster')
plt.show()

# Further analysis: Profiling the Clusters
# Grouping by clusters and computing mean and standard deviation for each RFM feature
cluster_profiles = rfm_df.groupby('Cluster').agg({
    'Recency': ['mean', 'std'],
    'Frequency': ['mean', 'std'],
    'Monetary': ['mean', 'std']
}).reset_index()

print("\nCluster Profiles:")
print(cluster_profiles)

# We can also create a more visual representation of the clusters
# For example, plotting the clusters in a 3D space using the RFM features
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(rfm_df['Recency'], rfm_df['Frequency'], rfm_df['Monetary'], c=rfm_df['Cluster'], cmap='Set1', s=100)
ax.set_xlabel('Recency')
ax.set_ylabel('Frequency')
ax.set_zlabel('Monetary')
ax.set_title('3D Visualization of RFM Clusters')
plt.show()

# Customer Profiling for Marketing Strategies
# Now that we have customer segments (clusters), we can associate each cluster with a marketing strategy.
# For example:
marketing_strategies = {
    0: "Low-Engagement: Target with re-engagement campaigns",
    1: "High-Engagement: Reward loyalty and increase retention",
    2: "Moderate-Engagement: Target with seasonal offers"
}

rfm_df['Marketing Strategy'] = rfm_df['Cluster'].map(marketing_strategies)

# Display the updated RFM DataFrame with marketing strategies
print("\nRFM DataFrame with Marketing Strategy:")
print(rfm_df[['Customer ID', 'Cluster', 'Marketing Strategy']].head())

# You could also build a more advanced predictive model for customer behavior (churn prediction, sales forecasting)
# For example, using Gradient Boosting Regressor for predicting future purchase amount based on RFM features

# Train a Gradient Boosting Regressor to predict future purchase value (Monetary)
X = rfm_df[['Recency', 'Frequency', 'Monetary']]
y = rfm_df['Monetary']  # This could be future spending or another target variable if available

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the Gradient Boosting model
gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb_model.fit(X_train, y_train)

# Predictions
y_pred_gb = gb_model.predict(X_test)

# Evaluate the model
mae_gb = mean_absolute_error(y_test, y_pred_gb)
r2_gb = r2_score(y_test, y_pred_gb)
print(f'Gradient Boosting - Mean Absolute Error: {mae_gb}')
print(f'Gradient Boosting - R^2 Score: {r2_gb}')

# Feature Importance: Understand which RFM features are most important for predictions
importances = gb_model.feature_importances_
features = ['Recency', 'Frequency', 'Monetary']
feature_importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print("\nFeature Importance:")
print(feature_importance_df)

# Plotting the feature importances with a hue
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df, hue='Feature', palette='viridis', legend=False)
plt.title('Feature Importance - Gradient Boosting Model')
plt.show()
